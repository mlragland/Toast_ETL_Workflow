# üç¥ Toast ETL Pipeline - Comprehensive Backfill Strategy

## **Executive Summary**

This document outlines the complete backfill strategy for the Toast ETL Pipeline, implementing date-by-date processing with intelligent business closure detection. The strategy processes **432+ available dates** from April 2024 to June 2025, ensuring complete data coverage while maintaining data integrity.

---

## **üìä Strategy Overview**

### **Core Principles**
1. **Date-by-Date Processing**: Each date processed individually through complete ETL pipeline
2. **Business Closure Detection**: Automatic detection and handling of closure dates
3. **Complete Date Coverage**: Every date gets either real data or closure records
4. **Duplicate Prevention**: Skip already processed dates automatically
5. **Fault Tolerance**: Robust error handling and retry mechanisms
6. **Operational Insights**: Track closure patterns for business intelligence

### **Key Benefits**
- ‚úÖ **100% Date Coverage**: No gaps in time series data
- ‚úÖ **Data Integrity**: Each date processed through full validation
- ‚úÖ **Maintenance-Free**: No manual holiday calendar management
- ‚úÖ **Self-Adapting**: Works for any business size with configurable thresholds
- ‚úÖ **Audit Trail**: Complete processing history and closure reasons
- ‚úÖ **Performance Optimized**: Parallel processing with configurable concurrency

---

## **üèóÔ∏è Architecture Components**

### **1. BackfillManager (Core Orchestrator)**
```python
class BackfillManager:
    """Manages historical data backfill operations with closure detection."""
    
    def __init__(self, max_workers=3, batch_size=10, skip_existing=True):
        self.sftp_extractor = SFTPExtractor()
        self.transformer = ToastDataTransformer()
        self.loader = BigQueryLoader()
        self.business_calendar = BusinessCalendar()
```

**Key Features:**
- Parallel processing with configurable worker threads
- Batch processing for optimal performance
- Automatic duplicate detection and skipping
- Comprehensive statistics tracking
- Business closure integration

### **2. BusinessCalendar (Closure Detection)**
```python
class BusinessCalendar:
    """Detects business closures using data-driven thresholds."""
    
    def should_process_as_closure(self, date: str, file_analysis: dict):
        """Returns (is_closure, reason, closure_records)"""
        
        # Configurable thresholds
        if file_analysis['files_found'] == 0:
            return True, 'no_files', self.generate_closure_records(date)
        
        if file_analysis['total_records'] < self.min_records_threshold:
            return True, 'low_activity', self.generate_closure_records(date)
```

**Detection Logic:**
- **No Files**: Zero files found on SFTP for date
- **Low Activity**: Below minimum record threshold (default: 10-15 records)
- **Insufficient Files**: Below minimum file count (default: 4-5 files)
- **Low Sales**: Below minimum sales threshold (default: $50-100)

### **3. SFTP Analysis Engine**
```python
def _analyze_sftp_files(self, date: str) -> Dict[str, Any]:
    """Analyze SFTP files without downloading to determine activity level."""
    
    files_found = self.sftp_extractor.list_files_for_date(date)
    total_records = 0
    
    for file_path in files_found:
        file_info = self.sftp_extractor.get_file_info(file_path)
        estimated_records = max(0, (file_info['size'] // 100) - 1)
        total_records += estimated_records
    
    return {
        'total_records': total_records,
        'files_found': len(files_found),
        'has_meaningful_data': total_records >= 10 and len(files_found) >= 4
    }
```

---

## **üîÑ Processing Workflow**

### **Phase 1: Discovery and Planning**
```bash
# 1. Scan SFTP for available dates
available_dates = backfill_manager.get_available_sftp_dates()
# Result: ['20240404', '20240405', ..., '20250609'] (432+ dates)

# 2. Check already processed dates
processed_dates = backfill_manager.get_processed_dates()
# Query BigQuery for existing data

# 3. Filter to new dates only
dates_to_process = backfill_manager.filter_dates_to_process(available_dates)
# Skip duplicates automatically
```

### **Phase 2: Date-by-Date Processing**
```python
def process_single_date(self, date: str) -> Dict[str, Any]:
    """Complete processing workflow for a single date."""
    
    # Step 1: Analyze SFTP files (without downloading)
    file_analysis = self._analyze_sftp_files(date)
    
    # Step 2: Business closure detection
    is_closure, reason, closure_records = self.business_calendar.should_process_as_closure(
        date, file_analysis
    )
    
    if is_closure:
        # Step 3a: Process as closure
        self._load_closure_records(date, closure_records, reason)
        return {'status': 'closure_processed', 'reason': reason}
    else:
        # Step 3b: Process as normal business day
        # Download ‚Üí Transform ‚Üí Load ‚Üí Validate
        return self._process_normal_business_day(date, file_analysis)
```

### **Phase 3: Parallel Batch Processing**
```python
def process_date_batch(self, dates: List[str]) -> List[Dict]:
    """Process multiple dates in parallel for optimal performance."""
    
    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
        future_to_date = {
            executor.submit(self.process_single_date, date): date 
            for date in dates
        }
        
        results = []
        for future in as_completed(future_to_date):
            result = future.result()
            results.append(result)
            self._update_statistics(result)
    
    return results
```

---

## **üìà Business Closure Handling**

### **Detection Scenarios**

#### **Scenario 1: Holiday Closure (Christmas Day)**
```python
# File Analysis Result:
{
    'files_found': 0,
    'total_records': 0,
    'has_meaningful_data': False
}

# Detection: is_closure=True, reason='no_files'
# Action: Generate closure records for all 7 tables
```

#### **Scenario 2: Low Activity Day (New Year's Day)**
```python
# File Analysis Result:
{
    'files_found': 3,
    'total_records': 8,
    'has_meaningful_data': False
}

# Detection: is_closure=True, reason='low_activity'
# Action: Generate closure records with metadata
```

#### **Scenario 3: Power Outage**
```python
# File Analysis Result:
{
    'files_found': 2,
    'total_records': 5,
    'has_meaningful_data': False
}

# Detection: is_closure=True, reason='insufficient_files'
# Action: Generate closure records with operational context
```

### **Closure Record Generation**
```python
def generate_closure_records(self, date: str, reason: str) -> Dict[str, DataFrame]:
    """Generate zero records for all tables with closure metadata."""
    
    closure_records = {}
    processing_date = datetime.strptime(date, '%Y%m%d').date()
    
    for table_name in self.table_schemas:
        # Create single closure record per table
        closure_record = {
            'order_id': f'CLOSURE_{date}',
            'location': 'Business Closed',
            'total': 0.0,
            'processing_date': processing_date,
            'closure_indicator': True,
            'closure_reason': reason,
            'created_date': processing_date
        }
        
        # Convert to DataFrame matching table schema
        df = pd.DataFrame([closure_record])
        closure_records[table_name] = df
    
    return closure_records
```

---

## **üöÄ Implementation Guide**

### **Step 1: Environment Setup**
```bash
# Set environment variables
export PROJECT_ID=toast-analytics-444116
export DATASET_ID=toast_analytics
export ENVIRONMENT=production

# Verify BigQuery schema updates (closure fields added)
python update_tables_for_closure_detection.py
```

### **Step 2: Configuration**
```python
# Configure backfill parameters
backfill_manager = BackfillManager(
    max_workers=3,          # Balanced for SFTP stability
    batch_size=10,          # Optimal batch size for monitoring
    skip_existing=True,     # Skip already processed dates
    validate_data=False     # Skip validation for speed in bulk processing
)

# Configure closure detection thresholds
business_calendar = BusinessCalendar(
    min_records_threshold=15,    # Minimum records for normal day
    min_files_threshold=5,       # Minimum files for normal day
    min_sales_threshold=100.0    # Minimum sales for normal day
)
```

### **Step 3: Execution Options**

#### **Option A: Complete Historical Backfill**
```bash
# Process all 432+ available dates
python run_backfill.py --all

# Expected results:
# - Duration: 2-4 hours
# - Records: 15,000-25,000+
# - Closure dates: 50-80 dates
# - Success rate: >95%
```

#### **Option B: Date Range Backfill**
```bash
# Process specific date range
python run_backfill.py --start-date 20240404 --end-date 20240430

# Process May 2025 (recent data)
python run_backfill.py --start-date 20250501 --end-date 20250531
```

#### **Option C: Specific Dates**
```bash
# Process individual dates
python run_backfill.py --dates 20241225 20250101 20250704

# Process failed dates from previous run
python run_backfill.py --dates $(cat failed_dates.txt)
```

### **Step 4: Monitoring and Validation**
```python
# Real-time progress monitoring
def monitor_backfill_progress():
    """Monitor backfill progress in real-time."""
    
    while backfill_running:
        stats = backfill_manager.get_current_stats()
        
        progress = stats['processed_dates'] / stats['total_dates'] * 100
        print(f"Progress: {progress:.1f}% - "
              f"Processed: {stats['processed_dates']}, "
              f"Failed: {stats['failed_dates']}, "
              f"Closures: {stats['closure_dates']}, "
              f"Records: {stats['total_records']:,}")
        
        time.sleep(30)  # Update every 30 seconds
```

---

## **üìä Expected Results**

### **Processing Statistics**
```
üéâ HISTORICAL BACKFILL COMPLETE!
======================================
üìÖ Total dates processed: 432
‚úÖ Successful dates: 380 (87.9%)
üè¢ Closure dates: 45 (10.4%)
‚ùå Failed dates: 7 (1.6%)
üìä Total records loaded: 22,847
‚è±Ô∏è  Total duration: 3h 24m
üéØ Success rate: 98.4%
```

### **Closure Detection Results**
```
üè¢ BUSINESS CLOSURE ANALYSIS
======================================
üìä Total closure dates detected: 45

Closure Reasons:
‚Ä¢ no_files: 28 dates (62.2%)
‚Ä¢ low_activity: 12 dates (26.7%)
‚Ä¢ insufficient_files: 5 dates (11.1%)

Seasonal Patterns:
‚Ä¢ December 2024: 8 closure dates
‚Ä¢ January 2025: 6 closure dates
‚Ä¢ Holiday periods: 15 closure dates
‚Ä¢ Weekends: 22 closure dates
```

### **Data Quality Metrics**
```
üìà DATA QUALITY SUMMARY
======================================
‚úÖ Date Coverage: 100% (no gaps)
‚úÖ Schema Compliance: 100%
‚úÖ Closure Indicators: All dates marked
‚úÖ Audit Trail: Complete processing history
‚úÖ Dashboard Ready: All metrics exclude closures
```

---

## **üîß Troubleshooting Guide**

### **Common Issues and Solutions**

#### **Issue 1: SFTP Connection Failures**
```bash
# Symptoms: "SFTP command failed with return code 255"
# Solution: Verify SSH key permissions and connectivity
chmod 600 ~/.ssh/toast_ssh
ssh -i ~/.ssh/toast_ssh LoveExportUser@s-9b0f88558b264dfda.server.transfer.us-east-1.amazonaws.com
```

#### **Issue 2: BigQuery Schema Mismatches**
```bash
# Symptoms: "Field 'closure_indicator' not found"
# Solution: Run schema update script
python update_tables_for_closure_detection.py
```

#### **Issue 3: Memory Issues with Large Batches**
```python
# Symptoms: Out of memory errors
# Solution: Reduce batch size and max workers
backfill_manager = BackfillManager(
    max_workers=2,    # Reduce from 3
    batch_size=5      # Reduce from 10
)
```

#### **Issue 4: High Failure Rate**
```bash
# Symptoms: >10% failure rate
# Solution: Check failed dates and retry individually
python run_backfill.py --dates $(python -c "
import json
with open('backfill_log.json') as f:
    data = json.load(f)
    print(' '.join(data['failed_date_list']))
")
```

---

## **üìã Production Deployment Checklist**

### **Pre-Deployment**
- [x] ‚úÖ BigQuery schema updated with closure fields
- [x] ‚úÖ SFTP credentials configured and tested
- [x] ‚úÖ Environment variables set correctly
- [x] ‚úÖ Business closure thresholds configured
- [x] ‚úÖ Monitoring and alerting configured
- [x] ‚úÖ Backup strategy in place

### **Deployment**
- [x] ‚úÖ Run test backfill on small date range
- [x] ‚úÖ Validate closure detection accuracy
- [x] ‚úÖ Verify dashboard excludes closure records
- [x] ‚úÖ Test parallel processing performance
- [x] ‚úÖ Confirm data quality metrics

### **Post-Deployment**
- [ ] Monitor initial backfill progress
- [ ] Validate final data counts and quality
- [ ] Update documentation with actual results
- [ ] Train operations team on monitoring
- [ ] Schedule regular maintenance tasks

---

## **üéØ Success Metrics**

### **Technical Metrics**
- **Date Coverage**: 100% (no gaps in time series)
- **Processing Success Rate**: >95%
- **Data Quality Score**: >98%
- **Performance**: <5 minutes per date average
- **Closure Detection Accuracy**: >90%

### **Business Metrics**
- **Dashboard Accuracy**: KPIs exclude closure days automatically
- **Operational Insights**: Clear closure pattern visibility
- **Data Integrity**: Complete audit trail for all dates
- **Maintenance Overhead**: Zero manual holiday management
- **Scalability**: Handles any business size with configuration

---

## **üìö Additional Resources**

### **Key Files**
- `src/backfill/backfill_manager.py` - Core backfill orchestrator
- `src/validators/business_calendar.py` - Closure detection logic
- `run_backfill.py` - Command-line interface
- `update_tables_for_closure_detection.py` - Schema migration
- `test_closure_detection_strategy.py` - Validation tests

### **API Endpoints**
- `GET /api/backfill` - Backfill status and history
- `POST /api/backfill/run` - Trigger backfill process
- `GET /api/analytics/closure-summary` - Closure analysis
- `GET /api/analytics/daily-trends` - Trends excluding closures

### **Monitoring Queries**
```sql
-- Check backfill progress
SELECT 
    DATE(created_date) as process_date,
    COUNT(*) as record_count,
    SUM(CASE WHEN closure_indicator = true THEN 1 ELSE 0 END) as closure_records
FROM `toast-analytics-444116.toast_analytics.order_details`
GROUP BY DATE(created_date)
ORDER BY process_date DESC
LIMIT 30;

-- Analyze closure patterns
SELECT 
    closure_reason,
    COUNT(*) as closure_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage
FROM `toast-analytics-444116.toast_analytics.order_details`
WHERE closure_indicator = true
GROUP BY closure_reason
ORDER BY closure_count DESC;
```

---

## **üèÜ Conclusion**

This comprehensive backfill strategy provides a robust, scalable, and maintenance-free solution for processing historical Toast ETL data. The date-by-date approach with intelligent business closure detection ensures complete data coverage while maintaining operational insights and data integrity.

**Key Achievements:**
- ‚úÖ **Complete Implementation**: All components working together seamlessly
- ‚úÖ **Production Ready**: Tested and validated with real data
- ‚úÖ **Self-Maintaining**: No manual intervention required
- ‚úÖ **Business Intelligence**: Operational closure insights included
- ‚úÖ **Future-Proof**: Scales with business growth and changes

The strategy is now ready for production deployment and will provide reliable, comprehensive historical data processing for the Toast ETL Pipeline. 